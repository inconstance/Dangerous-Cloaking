# Dangerous-Cloaking

- T-shirt (Cloaking attack) <br>
  [Baidu Netdisk](https://pan.baidu.com/s/1Ndb5WD3eoph0WJvbb-axTw)  Codeï¼š4pdi <br>
  [Google Drive](https://drive.google.com/file/d/1oQm2JcUe3SP4xJT8maNx-Spe13Xciegw/view?usp=sharing) <br>
  **Model**: YOLOv3, [YOLOv4](https://github.com/bubbliiiing/yolov4-pytorch/releases/tag/v2.0), CenterNet <br>
  **Dataset**: Pascal Voc dataset 20 classes + home-made dataset, 14,593 sheets in total, of which 14,041 sheets of pascal voc, poisoning rate between 0.14% and 3.4% <br>
  **Clean Baseline CDA**: 85.15% <br>
  **Backdoored Model CDA** : around 85% <br>
  **16 Test videos Avg ASR** : between 80~100% <br>
- Hat (Cloaking Attack) <br>
  [Baidu Netdisk](https://pan.baidu.com/s/14DZdowoKQxKiX61SDr1EKA) Code: jqpt <br>
  [Google Drive](https://drive.google.com/file/d/1liMER9ciCnhS_Z-vt3-ANx1euwGOTbvc/view?usp=sharing) <br> 
  **Model** : YOLOv4 <br>
  **Dataset**: Pascal Voc dataset 20 classes + homemade dataset, actual total 15,910 sheets, of which 14,041 sheets of pascal voc, experimenting with a maximum of 3.4% poisoning rate <br>
  **Clean Baseline CDA**: 85.15% <br>
  **Backdoored Model CDA** : 86.8% <br>
  **8 Test videos Avg ASR** : 68.62% <br>

  # Privacy
  The two home-made datasets show faces, where the training set is mosaicked for privacy, which can have a slight effect on the effectiveness of the attack. All publicly available datasets should only be used for academic research, so if you need to display any results with faces, please be careful to use mosaics. If you do require a dataset without mosaics, please contact us and state the purpose. The person appearing in the dataset has the right to request that their photograph be removed from that dataset at any time.
